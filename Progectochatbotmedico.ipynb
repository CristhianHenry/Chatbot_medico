{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progectochatbotmedico.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4ilWFdAiKghGN0IPxASEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CristhianHenry/Chatbot_medico/blob/main/Progectochatbotmedico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99tRfifK-h2C",
        "outputId": "99990a7d-30e7-4527-fc70-a91e887cb3cd"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127300 sha256=276baf03580cb347e2a74593bdaf3a7adf790c2ddff09ff69a246bc978ce1eba\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3gdG2FzD7He",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa901b4-5076-4ac9-b043-b9987eacf8e4"
      },
      "source": [
        "import nltk #para trabajar con procesamiento de lenguaje natural\n",
        "from nltk.stem.lancaster import LancasterStemmer #transformar palabras\n",
        "stemmer = LancasterStemmer() #objeto\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import json\n",
        "import random #escoger una respuesta aleatoria"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9S-j0aCD_qK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47f29969-2a1c-4551-d850-2139322e292f"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM8BBEfuEGkb"
      },
      "source": [
        "with open(\"cont.json\", encoding='utf-8') as archivo:#abrimos contenido\n",
        "  datos = json.load(archivo)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtxA4SubErCt"
      },
      "source": [
        "palabras=[] #guarda toda las palabras \n",
        "tags=[]          #guarda los tags \n",
        "auxX=[]      #guarda las preguntas\n",
        "auxY=[]    #guarda los tags para cada pregunta"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0beIyxbENEU",
        "outputId": "ea9fb4f1-309f-4de7-c4a2-f9149cb130b2"
      },
      "source": [
        "for contenido in datos[\"contenido\"]:     #entra en el contenidos del json\n",
        "  for patrones in contenido[\"patrones\"]: #busca los patrones o preguntas del json\n",
        "    auxPalabra = nltk.word_tokenize(patrones)#tokeniza los patrones y las guarda temporalmente en auxPalabra\n",
        "    palabras.extend(auxPalabra)     #auxPalabra tokenoizada se guarda en palabras\n",
        "    auxX.append(auxPalabra)          #para guardar las palabras como frases\n",
        "    auxY.append(contenido[\"tag\"])  #para guardar tags repetidos\n",
        "\n",
        "    if contenido[\"tag\"] not in tags:     #en la lista tags se guardan por una sola vez los tags a diferencia de en auxY\n",
        "      tags.append(contenido[\"tag\"])\n",
        "\n",
        "print(palabras)     # muestra las palabras no procesadas\n",
        "print(auxX)          # muestra las frases de preguntas en tokens\n",
        "print(auxY)        # muestra los tags repetidos por cada frase\n",
        "print(tags)         #muestra los tags"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hola', 'te', 'saludo', 'que', 'tal', 'hello', 'ola', 'adios', 'hasta', 'luego', 'nos', 'vemos', 'cuidate', 'bye', 'como', 'te', 'llamas', 'cual', 'es', 'tu', 'nombre', 'con', 'quien', 'hablo', 'como', 'es', 'tu', 'nombre', 'quien', 'eres', 'como', 'estas', 'que', 'haces', 'a', 'que', 'te', 'dedicas', 'que', 'estas', 'haciendo', 'que', 'eres', 'que', 'sabes', 'hacer', 'me', 'siento', 'enfermo', 'estoy', 'mal', 'me', 'duele', 'la', 'cabeza', 'siento', 'dolor', 'de', 'cabeza', 'tengo', 'una', 'jaqueca', 'cabesa', 'tengo', 'un', 'dolor', 'de', 'cabeza', 'migrañoso', 'migrañoso', 'migraña', 'tengo', 'un', 'dolor', 'de', 'cabeza', 'en', 'racimo', 'racimo', 'por', 'racimo', 'rasimo', 'tengo', 'dolor', 'de', 'cabeza', 'por', 'tensión', 'tension', 'por', 'tension', 'tencion', 'sabes', 'multiplicar', 'dividir', 'sumar', 'o', 'restar', 'sabes', 'hacer', 'operaciones', 'matematicas', 'sabes', 'que', 'hora', 'es', 'como', 'esta', 'el', 'clima', 'tienes', 'vida', 'tienes', 'sentimientos', 'cuantos', 'años', 'tienes', 'quien', 'es', 'tu', 'creador', 'quien', 'te', 'diseño', 'eres', 'un', 'chatbot', 'eres', 'un', 'robot', 'eres', 'un', 'bot']\n",
            "[['Hola'], ['te', 'saludo'], ['que', 'tal'], ['hello'], ['ola'], ['adios'], ['hasta', 'luego'], ['nos', 'vemos'], ['cuidate'], ['bye'], ['como', 'te', 'llamas'], ['cual', 'es', 'tu', 'nombre'], ['con', 'quien', 'hablo'], ['como', 'es', 'tu', 'nombre'], ['quien', 'eres'], ['como', 'estas'], ['que', 'haces'], ['a', 'que', 'te', 'dedicas'], ['que', 'estas', 'haciendo'], ['que', 'eres'], ['que', 'sabes', 'hacer'], ['me', 'siento', 'enfermo'], ['estoy', 'mal'], ['me', 'duele', 'la', 'cabeza'], ['siento', 'dolor', 'de', 'cabeza'], ['tengo', 'una', 'jaqueca'], ['cabesa'], ['tengo', 'un', 'dolor', 'de', 'cabeza', 'migrañoso'], ['migrañoso'], ['migraña'], ['tengo', 'un', 'dolor', 'de', 'cabeza', 'en', 'racimo'], ['racimo'], ['por', 'racimo'], ['rasimo'], ['tengo', 'dolor', 'de', 'cabeza', 'por', 'tensión'], ['tension'], ['por', 'tension'], ['tencion'], ['sabes', 'multiplicar', 'dividir', 'sumar', 'o', 'restar'], ['sabes', 'hacer', 'operaciones', 'matematicas'], ['sabes', 'que', 'hora', 'es'], ['como', 'esta', 'el', 'clima'], ['tienes', 'vida'], ['tienes', 'sentimientos'], ['cuantos', 'años', 'tienes'], ['quien', 'es', 'tu', 'creador'], ['quien', 'te', 'diseño'], ['eres', 'un', 'chatbot'], ['eres', 'un', 'robot'], ['eres', 'un', 'bot']]\n",
            "['saludo', 'saludo', 'saludo', 'saludo', 'saludo', 'despedida', 'despedida', 'despedida', 'despedida', 'despedida', 'nombre', 'nombre', 'nombre', 'nombre', 'nombre', 'estado', 'estado', 'estado', 'estado', 'estado', 'estado', 'restado', 'restado', 'rcabeza', 'rcabeza', 'rcabeza', 'rcabeza', 'rIecabeza', 'rIecabeza', 'rIecabeza', 'rIIecabeza', 'rIIecabeza', 'rIIecabeza', 'rIIecabeza', 'rIIIecabeza', 'rIIIecabeza', 'rIIIecabeza', 'rIIIecabeza', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rnegativas', 'rafirmativas', 'rafirmativas', 'rafirmativas']\n",
            "['saludo', 'despedida', 'nombre', 'estado', 'restado', 'rcabeza', 'rIecabeza', 'rIIecabeza', 'rIIIecabeza', 'rnegativas', 'rafirmativas']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9TBLdgFDvD",
        "outputId": "fee72b55-67dc-4a9e-b897-c3956e0fe379"
      },
      "source": [
        "palabras = [stemmer.stem(w.lower()) for w in palabras if w!=\"?\"] #para pasar las palabras a minusculas \n",
        "palabras = sorted(list(set(palabras)))#para ordenas las palabras\n",
        "tags = sorted(tags) #ordeno los tags  \n",
        "#listas la procesadas\n",
        "print(palabras)\n",
        "print(tags)         \n",
        "print(auxX)    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'adio', 'año', 'bot', 'bye', 'cabes', 'cabez', 'chatbot', 'clim', 'como', 'con', 'cread', 'cual', 'cuanto', 'cuid', 'de', 'dedica', 'diseño', 'dividir', 'dol', 'duel', 'el', 'en', 'enfermo', 'er', 'es', 'est', 'esta', 'estoy', 'hablo', 'hac', 'haciendo', 'hast', 'hello', 'hol', 'hor', 'jaquec', 'la', 'llama', 'luego', 'mal', 'matematica', 'me', 'migrañ', 'migrañoso', 'multiplic', 'nombr', 'nos', 'o', 'ol', 'operac', 'por', 'que', 'qui', 'racimo', 'rasimo', 'rest', 'robot', 'sab', 'saludo', 'sentimiento', 'siento', 'sum', 'tal', 'te', 'tend', 'tengo', 'tensión', 'tent', 'tien', 'tu', 'un', 'vemo', 'vid']\n",
            "['despedida', 'estado', 'nombre', 'rIIIecabeza', 'rIIecabeza', 'rIecabeza', 'rafirmativas', 'rcabeza', 'restado', 'rnegativas', 'saludo']\n",
            "[['Hola'], ['te', 'saludo'], ['que', 'tal'], ['hello'], ['ola'], ['adios'], ['hasta', 'luego'], ['nos', 'vemos'], ['cuidate'], ['bye'], ['como', 'te', 'llamas'], ['cual', 'es', 'tu', 'nombre'], ['con', 'quien', 'hablo'], ['como', 'es', 'tu', 'nombre'], ['quien', 'eres'], ['como', 'estas'], ['que', 'haces'], ['a', 'que', 'te', 'dedicas'], ['que', 'estas', 'haciendo'], ['que', 'eres'], ['que', 'sabes', 'hacer'], ['me', 'siento', 'enfermo'], ['estoy', 'mal'], ['me', 'duele', 'la', 'cabeza'], ['siento', 'dolor', 'de', 'cabeza'], ['tengo', 'una', 'jaqueca'], ['cabesa'], ['tengo', 'un', 'dolor', 'de', 'cabeza', 'migrañoso'], ['migrañoso'], ['migraña'], ['tengo', 'un', 'dolor', 'de', 'cabeza', 'en', 'racimo'], ['racimo'], ['por', 'racimo'], ['rasimo'], ['tengo', 'dolor', 'de', 'cabeza', 'por', 'tensión'], ['tension'], ['por', 'tension'], ['tencion'], ['sabes', 'multiplicar', 'dividir', 'sumar', 'o', 'restar'], ['sabes', 'hacer', 'operaciones', 'matematicas'], ['sabes', 'que', 'hora', 'es'], ['como', 'esta', 'el', 'clima'], ['tienes', 'vida'], ['tienes', 'sentimientos'], ['cuantos', 'años', 'tienes'], ['quien', 'es', 'tu', 'creador'], ['quien', 'te', 'diseño'], ['eres', 'un', 'chatbot'], ['eres', 'un', 'robot'], ['eres', 'un', 'bot']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9_6bxUrEzT1",
        "outputId": "61e4c9bc-f7ed-40e1-c3f3-89eff4637875"
      },
      "source": [
        "entrenamiento=[] #lista vacia que se utilizara para la cubeta de 0 y 1\n",
        "salida=[]        #\n",
        "salidaVacia=[0 for _ in range(len(tags))]#representa el tamaño de los tags\n",
        "for x, documento in enumerate(auxX): #se busca las frases de preguntas tokenizadas y se guarda las frases en el documento donde se asignaran posiciones como por ejemplo el documento = hola y la x=0 todo en la lista\n",
        "  cubeta=[]#alli es donde se volcará la cubeta\n",
        "  auxPalabra=[stemmer.stem(w.lower()) for w in documento]#las frases patron se ponen en minuscula  y guardando en auxPalabra\n",
        "  for w in palabras: #se busca cohincidencia entre las palabras para poder ubicarlo en la lista  \n",
        "    if w in auxPalabra:   #si encuentra entonces agrega al cubeta asi se van creando una lista \n",
        "      cubeta.append(1)#\n",
        "    else:#\n",
        "      cubeta.append(0)#    #si no encuentra no agrega nada a la lista\n",
        "  filaSalida = salidaVacia[:] #para asignar salidavacia a la filasalida\n",
        "  filaSalida[tags.index(auxY[x])] = 1 #coloca 1 a la lista de tag que le encontró\n",
        "\n",
        "  entrenamiento.append(cubeta) #agrega las listas cubeta\n",
        "  salida.append(filaSalida)   #agrefa las listas \n",
        "print(auxPalabra) \n",
        "print(cubeta)            \n",
        "print(filaSalida)             \n",
        "print(entrenamiento)    \n",
        "print(salida)           \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['er', 'un', 'bot']\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
            "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0HqJZSOFWCN",
        "outputId": "9be39ccc-70ce-43fa-8962-03cb120fae3c"
      },
      "source": [
        "entrenamiento = numpy.array(entrenamiento)#pasa la lista entrenamiento a un array\n",
        "salida = numpy.array(salida)              #pasa la lsita salida a un array\n",
        "tensorflow.compat.v1.reset_default_graph()#limpia el graph por defecto\n",
        "#modelo red neuronal\n",
        "red = tflearn.input_data(shape=[None,len(entrenamiento[0])])#red con ninguna forma \n",
        "red = tflearn.fully_connected(red,73)#capas con neuronas\n",
        "red = tflearn.fully_connected(red,73)\n",
        "red = tflearn.fully_connected(red,len(salida[0]),activation=\"softmax\")#la salida de tags es con funcion de activacion softmax\n",
        "\n",
        "red = tflearn.regression(red)# regresion para obtener probaliidaides \n",
        "modelo = tflearn.DNN(red)# para entrenar el modelo con DNN\n",
        "modelo.fit(entrenamiento, salida, n_epoch=250, batch_size=192, show_metric=True)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "---------------------------------\n",
            "Run id: J75TR8\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 50\n",
            "Validation samples: 0\n",
            "--\n",
            "Training Step: 1  | time: 0.406s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 50/50\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m2.15802\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 002 | loss: 2.15802 - acc: 0.0540 -- iter: 50/50\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.35318\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 003 | loss: 2.35318 - acc: 0.2225 -- iter: 50/50\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.38475\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 004 | loss: 2.38475 - acc: 0.2956 -- iter: 50/50\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m2.39301\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 005 | loss: 2.39301 - acc: 0.2017 -- iter: 50/50\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m2.39281\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 006 | loss: 2.39281 - acc: 0.2778 -- iter: 50/50\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m2.39191\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 007 | loss: 2.39191 - acc: 0.3031 -- iter: 50/50\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m2.39066\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 008 | loss: 2.39066 - acc: 0.3239 -- iter: 50/50\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m2.38916\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 009 | loss: 2.38916 - acc: 0.3112 -- iter: 50/50\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m2.39123\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 010 | loss: 2.39123 - acc: 0.2256 -- iter: 50/50\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m2.38758\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 011 | loss: 2.38758 - acc: 0.2608 -- iter: 50/50\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m2.38981\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 012 | loss: 2.38981 - acc: 0.2065 -- iter: 50/50\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m2.38495\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 013 | loss: 2.38495 - acc: 0.2380 -- iter: 50/50\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m2.38723\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 014 | loss: 2.38723 - acc: 0.1897 -- iter: 50/50\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m2.38149\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 015 | loss: 2.38149 - acc: 0.2016 -- iter: 50/50\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m2.38409\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 016 | loss: 2.38409 - acc: 0.1935 -- iter: 50/50\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m2.37741\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 017 | loss: 2.37741 - acc: 0.1958 -- iter: 50/50\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m2.37965\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 018 | loss: 2.37965 - acc: 0.1973 -- iter: 50/50\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m2.37209\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 019 | loss: 2.37209 - acc: 0.1982 -- iter: 50/50\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m2.36557\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 020 | loss: 2.36557 - acc: 0.1988 -- iter: 50/50\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m2.35943\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 021 | loss: 2.35943 - acc: 0.1991 -- iter: 50/50\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m2.35326\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 022 | loss: 2.35326 - acc: 0.1994 -- iter: 50/50\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m2.34676\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 023 | loss: 2.34676 - acc: 0.1996 -- iter: 50/50\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m2.33976\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 024 | loss: 2.33976 - acc: 0.1997 -- iter: 50/50\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m2.33208\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 025 | loss: 2.33208 - acc: 0.1998 -- iter: 50/50\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.32363\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 026 | loss: 2.32363 - acc: 0.1945 -- iter: 50/50\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m2.31428\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 027 | loss: 2.31428 - acc: 0.2011 -- iter: 50/50\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m2.30396\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 028 | loss: 2.30396 - acc: 0.2058 -- iter: 50/50\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.29259\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 029 | loss: 2.29259 - acc: 0.2093 -- iter: 50/50\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.28011\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 030 | loss: 2.28011 - acc: 0.2118 -- iter: 50/50\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m2.26647\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 031 | loss: 2.26647 - acc: 0.2137 -- iter: 50/50\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m2.25162\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 032 | loss: 2.25162 - acc: 0.2196 -- iter: 50/50\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m2.23556\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 033 | loss: 2.23556 - acc: 0.2285 -- iter: 50/50\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m2.21829\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 034 | loss: 2.21829 - acc: 0.2352 -- iter: 50/50\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m2.19984\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 035 | loss: 2.19984 - acc: 0.2404 -- iter: 50/50\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m2.22774\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 036 | loss: 2.22774 - acc: 0.2281 -- iter: 50/50\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m2.19834\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 037 | loss: 2.19834 - acc: 0.2344 -- iter: 50/50\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m2.17054\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 038 | loss: 2.17054 - acc: 0.2394 -- iter: 50/50\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m2.14383\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 039 | loss: 2.14383 - acc: 0.2434 -- iter: 50/50\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m2.11786\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 040 | loss: 2.11786 - acc: 0.2465 -- iter: 50/50\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m2.09241\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 041 | loss: 2.09241 - acc: 0.2490 -- iter: 50/50\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m2.06736\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 042 | loss: 2.06736 - acc: 0.2474 -- iter: 50/50\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m2.04265\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 043 | loss: 2.04265 - acc: 0.2461 -- iter: 50/50\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m2.01823\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 044 | loss: 2.01823 - acc: 0.2450 -- iter: 50/50\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.99408\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 045 | loss: 1.99408 - acc: 0.2510 -- iter: 50/50\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.97016\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 046 | loss: 1.97016 - acc: 0.2658 -- iter: 50/50\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.94646\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 047 | loss: 1.94646 - acc: 0.2779 -- iter: 50/50\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.92294\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 048 | loss: 1.92294 - acc: 0.2879 -- iter: 50/50\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.89957\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 049 | loss: 1.89957 - acc: 0.2993 -- iter: 50/50\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.87631\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 050 | loss: 1.87631 - acc: 0.3087 -- iter: 50/50\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.85315\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 051 | loss: 1.85315 - acc: 0.3165 -- iter: 50/50\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.83004\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 052 | loss: 1.83004 - acc: 0.3201 -- iter: 50/50\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.80697\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 053 | loss: 1.80697 - acc: 0.3230 -- iter: 50/50\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.78392\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 054 | loss: 1.78392 - acc: 0.3226 -- iter: 50/50\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.76088\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 055 | loss: 1.76088 - acc: 0.3193 -- iter: 50/50\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.73786\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 056 | loss: 1.73786 - acc: 0.3194 -- iter: 50/50\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.71488\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 057 | loss: 1.71488 - acc: 0.3251 -- iter: 50/50\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.69194\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 058 | loss: 1.69194 - acc: 0.3325 -- iter: 50/50\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.66907\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 059 | loss: 1.66907 - acc: 0.3497 -- iter: 50/50\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.64629\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 060 | loss: 1.64629 - acc: 0.3669 -- iter: 50/50\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.60105\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 061 | loss: 1.60105 - acc: 0.3843 -- iter: 50/50\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.60105\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 062 | loss: 1.60105 - acc: 0.3966 -- iter: 50/50\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.57860\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 063 | loss: 1.57860 - acc: 0.4148 -- iter: 50/50\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.83154\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 064 | loss: 1.83154 - acc: 0.3779 -- iter: 50/50\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.77653\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 065 | loss: 1.77653 - acc: 0.4004 -- iter: 50/50\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.96549\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 066 | loss: 1.96549 - acc: 0.3638 -- iter: 50/50\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.89306\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 067 | loss: 1.89306 - acc: 0.3994 -- iter: 50/50\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.82907\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 068 | loss: 1.82907 - acc: 0.4302 -- iter: 50/50\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.77218\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 069 | loss: 1.77218 - acc: 0.4571 -- iter: 50/50\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.97294\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 070 | loss: 1.97294 - acc: 0.4136 -- iter: 50/50\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.89904\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 071 | loss: 1.89904 - acc: 0.4462 -- iter: 50/50\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.83347\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 072 | loss: 1.83347 - acc: 0.4748 -- iter: 50/50\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.77490\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 073 | loss: 1.77490 - acc: 0.4998 -- iter: 50/50\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.72221\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 074 | loss: 1.72221 - acc: 0.5240 -- iter: 50/50\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.67445\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 075 | loss: 1.67445 - acc: 0.5474 -- iter: 50/50\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.63084\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 076 | loss: 1.63084 - acc: 0.5680 -- iter: 50/50\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.59072\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 077 | loss: 1.59072 - acc: 0.5862 -- iter: 50/50\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.55353\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 078 | loss: 1.55353 - acc: 0.6023 -- iter: 50/50\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.51882\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 079 | loss: 1.51882 - acc: 0.6186 -- iter: 50/50\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.48621\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 080 | loss: 1.48621 - acc: 0.6331 -- iter: 50/50\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.45540\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 081 | loss: 1.45540 - acc: 0.6459 -- iter: 50/50\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.42612\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 082 | loss: 1.42612 - acc: 0.6573 -- iter: 50/50\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.39786\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 083 | loss: 1.39786 - acc: 0.6676 -- iter: 50/50\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.37050\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 084 | loss: 1.37050 - acc: 0.6768 -- iter: 50/50\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.34393\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 085 | loss: 1.34393 - acc: 0.6852 -- iter: 50/50\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.31807\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 086 | loss: 1.31807 - acc: 0.6946 -- iter: 50/50\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.29287\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 087 | loss: 1.29287 - acc: 0.7032 -- iter: 50/50\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.26824\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 088 | loss: 1.26824 - acc: 0.7109 -- iter: 50/50\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.24415\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 089 | loss: 1.24415 - acc: 0.7178 -- iter: 50/50\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.22055\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 090 | loss: 1.22055 - acc: 0.7240 -- iter: 50/50\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.19740\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 091 | loss: 1.19740 - acc: 0.7316 -- iter: 50/50\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.15236\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 092 | loss: 1.15236 - acc: 0.7384 -- iter: 50/50\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.15236\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 093 | loss: 1.15236 - acc: 0.7446 -- iter: 50/50\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.13041\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 094 | loss: 1.13041 - acc: 0.7501 -- iter: 50/50\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.10883\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 095 | loss: 1.10883 - acc: 0.7571 -- iter: 50/50\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.08759\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 096 | loss: 1.08759 - acc: 0.7634 -- iter: 50/50\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.04615\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 097 | loss: 1.04615 - acc: 0.7691 -- iter: 50/50\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.02594\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 098 | loss: 1.02594 - acc: 0.7787 -- iter: 50/50\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.02594\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 099 | loss: 1.02594 - acc: 0.7829 -- iter: 50/50\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.00605\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 100 | loss: 1.00605 - acc: 0.7886 -- iter: 50/50\n",
            "--\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.98650\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 101 | loss: 0.98650 - acc: 0.7886 -- iter: 50/50\n",
            "--\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.96727\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 102 | loss: 0.96727 - acc: 0.7937 -- iter: 50/50\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.94838\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 103 | loss: 0.94838 - acc: 0.7984 -- iter: 50/50\n",
            "--\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.92981\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 104 | loss: 0.92981 - acc: 0.8045 -- iter: 50/50\n",
            "--\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.91156\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 105 | loss: 0.91156 - acc: 0.7371 -- iter: 50/50\n",
            "--\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.25509\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 106 | loss: 1.25509 - acc: 0.7371 -- iter: 50/50\n",
            "--\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.20203\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 107 | loss: 1.20203 - acc: 0.7494 -- iter: 50/50\n",
            "--\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.15347\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 108 | loss: 1.15347 - acc: 0.7624 -- iter: 50/50\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.10892\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 109 | loss: 1.10892 - acc: 0.7742 -- iter: 50/50\n",
            "--\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.47268\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 110 | loss: 1.47268 - acc: 0.7028 -- iter: 50/50\n",
            "--\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.39505\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 111 | loss: 1.39505 - acc: 0.7205 -- iter: 50/50\n",
            "--\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.72395\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 112 | loss: 1.72395 - acc: 0.6604 -- iter: 50/50\n",
            "--\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.62104\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 113 | loss: 1.62104 - acc: 0.6864 -- iter: 50/50\n",
            "--\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.90093\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 114 | loss: 1.90093 - acc: 0.6278 -- iter: 50/50\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.78100\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 115 | loss: 1.78100 - acc: 0.6550 -- iter: 50/50\n",
            "--\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m2.05607\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 116 | loss: 2.05607 - acc: 0.5995 -- iter: 50/50\n",
            "--\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.92194\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 117 | loss: 1.92194 - acc: 0.6315 -- iter: 50/50\n",
            "--\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.80194\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 118 | loss: 1.80194 - acc: 0.6604 -- iter: 50/50\n",
            "--\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.69449\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 119 | loss: 1.69449 - acc: 0.6863 -- iter: 50/50\n",
            "--\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.94214\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 120 | loss: 1.94214 - acc: 0.6257 -- iter: 50/50\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.82182\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 121 | loss: 1.82182 - acc: 0.6571 -- iter: 50/50\n",
            "--\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.71410\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 122 | loss: 1.71410 - acc: 0.6854 -- iter: 50/50\n",
            "--\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.61753\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 123 | loss: 1.61753 - acc: 0.7109 -- iter: 50/50\n",
            "--\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.53081\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 124 | loss: 1.53081 - acc: 0.7338 -- iter: 50/50\n",
            "--\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.45277\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 125 | loss: 1.45277 - acc: 0.7544 -- iter: 50/50\n",
            "--\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m1.70884\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 126 | loss: 1.70884 - acc: 0.6850 -- iter: 50/50\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m1.61315\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 127 | loss: 1.61315 - acc: 0.7105 -- iter: 50/50\n",
            "--\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.82910\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 128 | loss: 1.82910 - acc: 0.6494 -- iter: 50/50\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.72209\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 129 | loss: 1.72209 - acc: 0.6785 -- iter: 50/50\n",
            "--\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.62616\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 130 | loss: 1.62616 - acc: 0.7046 -- iter: 50/50\n",
            "--\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m1.54001\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 131 | loss: 1.54001 - acc: 0.7282 -- iter: 50/50\n",
            "--\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m1.46248\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 132 | loss: 1.46248 - acc: 0.7514 -- iter: 50/50\n",
            "--\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m1.59902\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 133 | loss: 1.59902 - acc: 0.7722 -- iter: 50/50\n",
            "--\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m1.51524\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 134 | loss: 1.51524 - acc: 0.7130 -- iter: 50/50\n",
            "--\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m1.51524\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 135 | loss: 1.51524 - acc: 0.7377 -- iter: 50/50\n",
            "--\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m1.37165\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 136 | loss: 1.37165 - acc: 0.7619 -- iter: 50/50\n",
            "--\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m1.37165\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 137 | loss: 1.37165 - acc: 0.7857 -- iter: 50/50\n",
            "--\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m1.49480\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 138 | loss: 1.49480 - acc: 0.7212 -- iter: 50/50\n",
            "--\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m1.49480\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 139 | loss: 1.49480 - acc: 0.7490 -- iter: 50/50\n",
            "--\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m1.42080\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 140 | loss: 1.42080 - acc: 0.7741 -- iter: 50/50\n",
            "--\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m1.35397\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 141 | loss: 1.35397 - acc: 0.7271 -- iter: 50/50\n",
            "--\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m1.49520\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 142 | loss: 1.49520 - acc: 0.7271 -- iter: 50/50\n",
            "--\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m1.42057\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 143 | loss: 1.42057 - acc: 0.7543 -- iter: 50/50\n",
            "--\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m1.35317\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 144 | loss: 1.35317 - acc: 0.7789 -- iter: 50/50\n",
            "--\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m1.35317\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 145 | loss: 1.35317 - acc: 0.8010 -- iter: 50/50\n",
            "--\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m1.58271\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 146 | loss: 1.58271 - acc: 0.7289 -- iter: 50/50\n",
            "--\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m1.49884\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 147 | loss: 1.49884 - acc: 0.7560 -- iter: 50/50\n",
            "--\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m1.42329\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 148 | loss: 1.42329 - acc: 0.7804 -- iter: 50/50\n",
            "--\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m1.35507\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 149 | loss: 1.35507 - acc: 0.8024 -- iter: 50/50\n",
            "--\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m1.29331\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 150 | loss: 1.29331 - acc: 0.8221 -- iter: 50/50\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.23722\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 151 | loss: 1.23722 - acc: 0.8399 -- iter: 50/50\n",
            "--\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.18611\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 152 | loss: 1.18611 - acc: 0.8559 -- iter: 50/50\n",
            "--\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.13939\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 153 | loss: 1.13939 - acc: 0.8703 -- iter: 50/50\n",
            "--\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.09651\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 154 | loss: 1.09651 - acc: 0.8833 -- iter: 50/50\n",
            "--\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.05701\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 155 | loss: 1.05701 - acc: 0.8950 -- iter: 50/50\n",
            "--\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.02048\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 156 | loss: 1.02048 - acc: 0.9055 -- iter: 50/50\n",
            "--\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.98656\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 157 | loss: 0.98656 - acc: 0.9149 -- iter: 50/50\n",
            "--\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.95494\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 158 | loss: 0.95494 - acc: 0.9234 -- iter: 50/50\n",
            "--\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.92534\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 159 | loss: 0.92534 - acc: 0.9311 -- iter: 50/50\n",
            "--\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.89753\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 160 | loss: 0.89753 - acc: 0.9380 -- iter: 50/50\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.87130\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 161 | loss: 0.87130 - acc: 0.9442 -- iter: 50/50\n",
            "--\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.84646\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 162 | loss: 0.84646 - acc: 0.9498 -- iter: 50/50\n",
            "--\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.82287\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 163 | loss: 0.82287 - acc: 0.9548 -- iter: 50/50\n",
            "--\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.80039\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 164 | loss: 0.80039 - acc: 0.9593 -- iter: 50/50\n",
            "--\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.77890\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 165 | loss: 0.77890 - acc: 0.9634 -- iter: 50/50\n",
            "--\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.75830\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 166 | loss: 0.75830 - acc: 0.9670 -- iter: 50/50\n",
            "--\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.73850\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 167 | loss: 0.73850 - acc: 0.9703 -- iter: 50/50\n",
            "--\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.71943\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 168 | loss: 0.71943 - acc: 0.9733 -- iter: 50/50\n",
            "--\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.70101\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 169 | loss: 0.70101 - acc: 0.9760 -- iter: 50/50\n",
            "--\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.68320\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 170 | loss: 0.68320 - acc: 0.9784 -- iter: 50/50\n",
            "--\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.66593\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 171 | loss: 0.66593 - acc: 0.9805 -- iter: 50/50\n",
            "--\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.93935\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 172 | loss: 0.93935 - acc: 0.9005 -- iter: 50/50\n",
            "--\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.93935\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 173 | loss: 0.93935 - acc: 0.9104 -- iter: 50/50\n",
            "--\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m1.26284\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 174 | loss: 1.26284 - acc: 0.8354 -- iter: 50/50\n",
            "--\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m1.11435\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 175 | loss: 1.11435 - acc: 0.8519 -- iter: 50/50\n",
            "--\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m1.11435\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 176 | loss: 1.11435 - acc: 0.8667 -- iter: 50/50\n",
            "--\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m1.05053\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 177 | loss: 1.05053 - acc: 0.8800 -- iter: 50/50\n",
            "--\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m1.27200\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 178 | loss: 1.27200 - acc: 0.8000 -- iter: 50/50\n",
            "--\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m1.53715\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 179 | loss: 1.53715 - acc: 0.8200 -- iter: 50/50\n",
            "--\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m1.53715\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 180 | loss: 1.53715 - acc: 0.7440 -- iter: 50/50\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m1.43086\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 181 | loss: 1.43086 - acc: 0.7696 -- iter: 50/50\n",
            "--\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m1.69108\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 182 | loss: 1.69108 - acc: 0.7026 -- iter: 50/50\n",
            "--\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m1.57016\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 183 | loss: 1.57016 - acc: 0.7324 -- iter: 50/50\n",
            "--\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m1.81770\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 184 | loss: 1.81770 - acc: 0.6731 -- iter: 50/50\n",
            "--\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m1.68540\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 185 | loss: 1.68540 - acc: 0.7058 -- iter: 50/50\n",
            "--\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m1.56700\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 186 | loss: 1.56700 - acc: 0.7352 -- iter: 50/50\n",
            "--\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m1.46093\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 187 | loss: 1.46093 - acc: 0.7617 -- iter: 50/50\n",
            "--\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m1.69722\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 188 | loss: 1.69722 - acc: 0.6975 -- iter: 50/50\n",
            "--\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m1.57917\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 189 | loss: 1.57917 - acc: 0.7278 -- iter: 50/50\n",
            "--\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m1.82931\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 190 | loss: 1.82931 - acc: 0.6610 -- iter: 50/50\n",
            "--\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m1.69955\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 191 | loss: 1.69955 - acc: 0.6949 -- iter: 50/50\n",
            "--\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m1.58351\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 192 | loss: 1.58351 - acc: 0.7254 -- iter: 50/50\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m1.47962\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 193 | loss: 1.47962 - acc: 0.7529 -- iter: 50/50\n",
            "--\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m1.68274\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 194 | loss: 1.68274 - acc: 0.6916 -- iter: 50/50\n",
            "--\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m1.56995\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 195 | loss: 1.56995 - acc: 0.7224 -- iter: 50/50\n",
            "--\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m1.46891\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 196 | loss: 1.46891 - acc: 0.7502 -- iter: 50/50\n",
            "--\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m1.37824\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 197 | loss: 1.37824 - acc: 0.7752 -- iter: 50/50\n",
            "--\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m1.29670\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 198 | loss: 1.29670 - acc: 0.7977 -- iter: 50/50\n",
            "--\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m1.22322\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 199 | loss: 1.22322 - acc: 0.8179 -- iter: 50/50\n",
            "--\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m1.41706\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 200 | loss: 1.41706 - acc: 0.7461 -- iter: 50/50\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m1.33138\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 201 | loss: 1.33138 - acc: 0.7715 -- iter: 50/50\n",
            "--\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m1.25420\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 202 | loss: 1.25420 - acc: 0.7943 -- iter: 50/50\n",
            "--\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m1.18450\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 203 | loss: 1.18450 - acc: 0.8149 -- iter: 50/50\n",
            "--\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m1.12140\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 204 | loss: 1.12140 - acc: 0.8334 -- iter: 50/50\n",
            "--\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m1.06411\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 205 | loss: 1.06411 - acc: 0.8501 -- iter: 50/50\n",
            "--\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m1.01195\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 206 | loss: 1.01195 - acc: 0.8651 -- iter: 50/50\n",
            "--\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.96429\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 207 | loss: 0.96429 - acc: 0.8786 -- iter: 50/50\n",
            "--\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.92061\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 208 | loss: 0.92061 - acc: 0.8907 -- iter: 50/50\n",
            "--\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.88044\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 209 | loss: 0.88044 - acc: 0.9016 -- iter: 50/50\n",
            "--\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m1.13243\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 210 | loss: 1.13243 - acc: 0.8295 -- iter: 50/50\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m1.06969\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 211 | loss: 1.06969 - acc: 0.8465 -- iter: 50/50\n",
            "--\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m1.33672\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 212 | loss: 1.33672 - acc: 0.7739 -- iter: 50/50\n",
            "--\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m1.47381\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 213 | loss: 1.47381 - acc: 0.7965 -- iter: 50/50\n",
            "--\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m1.47381\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 214 | loss: 1.47381 - acc: 0.7328 -- iter: 50/50\n",
            "--\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m1.62438\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 215 | loss: 1.62438 - acc: 0.7596 -- iter: 50/50\n",
            "--\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m1.51238\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 216 | loss: 1.51238 - acc: 0.6936 -- iter: 50/50\n",
            "--\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m1.51238\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 217 | loss: 1.51238 - acc: 0.7242 -- iter: 50/50\n",
            "--\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m1.41189\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 218 | loss: 1.41189 - acc: 0.7518 -- iter: 50/50\n",
            "--\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m1.32157\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 219 | loss: 1.32157 - acc: 0.7766 -- iter: 50/50\n",
            "--\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m1.24026\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 220 | loss: 1.24026 - acc: 0.7990 -- iter: 50/50\n",
            "--\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m1.39354\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 221 | loss: 1.39354 - acc: 0.8191 -- iter: 50/50\n",
            "--\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m1.39354\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 222 | loss: 1.39354 - acc: 0.7552 -- iter: 50/50\n",
            "--\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m1.30462\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 223 | loss: 1.30462 - acc: 0.7796 -- iter: 50/50\n",
            "--\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m1.22453\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 224 | loss: 1.22453 - acc: 0.8017 -- iter: 50/50\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m1.15223\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 225 | loss: 1.15223 - acc: 0.8215 -- iter: 50/50\n",
            "--\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m1.08682\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 226 | loss: 1.08682 - acc: 0.8394 -- iter: 50/50\n",
            "--\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m1.02748\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 227 | loss: 1.02748 - acc: 0.8554 -- iter: 50/50\n",
            "--\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m1.26264\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 228 | loss: 1.26264 - acc: 0.7859 -- iter: 50/50\n",
            "--\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m1.18503\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 229 | loss: 1.18503 - acc: 0.8073 -- iter: 50/50\n",
            "--\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m1.05144\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 230 | loss: 1.05144 - acc: 0.8266 -- iter: 50/50\n",
            "--\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m1.05144\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 231 | loss: 1.05144 - acc: 0.8439 -- iter: 50/50\n",
            "--\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.99382\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 232 | loss: 0.99382 - acc: 0.8595 -- iter: 50/50\n",
            "--\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.94138\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 233 | loss: 0.94138 - acc: 0.8736 -- iter: 50/50\n",
            "--\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.84967\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 234 | loss: 0.84967 - acc: 0.8862 -- iter: 50/50\n",
            "--\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.80941\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 235 | loss: 0.80941 - acc: 0.8976 -- iter: 50/50\n",
            "--\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.80941\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 236 | loss: 0.80941 - acc: 0.9078 -- iter: 50/50\n",
            "--\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.77232\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 237 | loss: 0.77232 - acc: 0.9170 -- iter: 50/50\n",
            "--\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.70623\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 238 | loss: 0.70623 - acc: 0.9253 -- iter: 50/50\n",
            "--\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.70623\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 239 | loss: 0.70623 - acc: 0.9328 -- iter: 50/50\n",
            "--\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.64905\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 240 | loss: 0.64905 - acc: 0.9395 -- iter: 50/50\n",
            "--\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.94663\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 241 | loss: 0.94663 - acc: 0.9456 -- iter: 50/50\n",
            "--\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.89051\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 242 | loss: 0.89051 - acc: 0.8630 -- iter: 50/50\n",
            "--\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m1.16343\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 243 | loss: 1.16343 - acc: 0.8767 -- iter: 50/50\n",
            "--\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m1.08488\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 244 | loss: 1.08488 - acc: 0.7970 -- iter: 50/50\n",
            "--\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m1.31991\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 245 | loss: 1.31991 - acc: 0.8173 -- iter: 50/50\n",
            "--\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m1.31991\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 246 | loss: 1.31991 - acc: 0.7516 -- iter: 50/50\n",
            "--\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m1.14072\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 247 | loss: 1.14072 - acc: 0.7764 -- iter: 50/50\n",
            "--\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m1.06423\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 248 | loss: 1.06423 - acc: 0.7988 -- iter: 50/50\n",
            "--\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m1.06423\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 249 | loss: 1.06423 - acc: 0.8189 -- iter: 50/50\n",
            "--\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.99518\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 250 | loss: 0.99518 - acc: 0.8370 -- iter: 50/50\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aP3wbJVFazE",
        "outputId": "00069bbd-e560-4ee6-e99c-87c46634ce31"
      },
      "source": [
        "\n",
        "def mainBot():\n",
        "  while True:\n",
        "    entrada = input(\"humano: \")\n",
        "    if entrada != \"adios\": #para salir \n",
        "      cubeta = [0 for _ in range(len(palabras))]#lleno el cubeta de 0 para reinicializar\n",
        "      entradaProcesada = nltk.word_tokenize(entrada)# se tokenizo la entrada\n",
        "      entradaProcesada = [stemmer.stem(palabra.lower()) for palabra in entradaProcesada]#stemizo la entrada\n",
        "      for palabraIndividual in entradaProcesada:    #buscar con palabra individual la lista palabras\n",
        "        for i,palabra in enumerate(palabras):  \n",
        "          if palabra == palabraIndividual:          #si encuentro cohincidencia entonces agrego al cubeta las posiciones\n",
        "            cubeta[i] = 1 #\n",
        "      resultados = modelo.predict([numpy.array(cubeta)])#Tomamos el modelo red como parámetro \n",
        "      if numpy.amax(resultados)>0.5: #para poder encontrar la palabra tiene que ser mayor 0.5\n",
        "        resultadoIndices = numpy.argmax(resultados)#  argmax retorna el INDICE del mayor elelmto\n",
        "\n",
        "        tag = tags[resultadoIndices]#asigna tag al cual tiene mas posiblidades \n",
        "        for tagAux in datos[\"contenido\"]: #busca el contenido del json\n",
        "          if tagAux[\"tag\"] == tag:        #en el json cuando encuentre al tag entonces asignara respuestas \n",
        "            respuesta = tagAux[\"respuestas\"]#\n",
        "\n",
        "        print(\"Bot: \",random.choice(respuesta))#elige aleatoriamente cualquera de esas posibles resuestas  \n",
        "\n",
        "      else:#si no hay certeza del mayor a 50% entonces responde \n",
        "        print(\"Bot: \",random.choice([\"no estoy muy seguro de la respuesta\",\"no tengo certerza de ello\"]))\n",
        "\n",
        "    else:\n",
        "      print(\"Bot: \",random.choice([\"hasta luego\",\"fue un gusto\"]))\n",
        "      break\n",
        "\n",
        "mainBot()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "humano: ola\n",
            "Bot:  Hola que tal!\n",
            "humano: me siento mal\n",
            "Bot:  está bien puedes decirme que esta mal ¿cuáles son tus síntomas?\n",
            "humano: me duele la cabesa\n",
            "Bot:  qué tipo de dolor de cabeza tiene : 1 Dolor de cabeza migrañoso: dolor punzante intenso en un solo lado de la cabeza. 2 Dolores de cabeza en racimo - dolores de cabeza severos y recurrentes 3 Dolor de cabeza por tensión - Ocasional\n",
            "humano: rasimo\n",
            "Bot:  síntoma observado. Los ataques tienden a ocurrir en grupos diarios que pueden persistir durante semanas o meses. Por lo general, tienen lugar a la misma hora del día, que a menudo puede ser un par de horas después de quedarse dormido por la noche\n",
            "humano: asios\n",
            "Bot:  no tengo certerza de ello\n",
            "humano: adios\n",
            "Bot:  fue un gusto\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}